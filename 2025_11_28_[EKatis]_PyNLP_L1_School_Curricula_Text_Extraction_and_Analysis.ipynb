{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vkreations/CCSO/blob/master/2025_11_28_%5BEKatis%5D_PyNLP_L1_School_Curricula_Text_Extraction_and_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Extraction and Analysis of L1 School Curricula with lemmatization**\n",
        "\n",
        "In this program, we will explore the fundamentals of text extraction and analysis using Python, with practical examples performed on Greek language texts extracted from education curricula.\n",
        "\n",
        "The goal of this program is to provide a step-by-step guide that helps you understand how to work with textual data, even if you have no prior experience with Python.\n",
        "The program calculates step-by-step the following metrics for each text/document, displays the results and also save them in Excel file:\n",
        "*   Basic Metrics: Total pages, characters, words, unique words.\n",
        "*   Cosine similarity.\n",
        "*   Top-100 words frequencies (before processing)\n",
        "*   Summary of POS words: nouns, adjectives, adverbs, verbs.\n",
        "*   Top-25 of POS words: nouns, adjectives, adverbs, verbs.\n",
        "*   POS words comparison.\n",
        "*   Top-25 Bigrams.\n",
        "\n",
        "First we will extract text from PDF documents, move on to analyzing and visualizing word frequencies, and then explore techniques such as text similarity analysis. By the end of this tutorial, you'll also learn how to apply linguistic tools like part-of-speech (POS) tagging and understand stop words and their importance in basic text analysis tasks.\n"
      ],
      "metadata": {
        "id": "ZgKR79vCvnhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 1. Uploading and Reading PDF Files\n",
        "\n",
        "In this section, we set up **the necessary tools to upload and read text from PDF files:**\n",
        "\n",
        "1. Install PyMuPDF: PyMuPDF is widely used for processing PDF files.\n",
        "\n",
        "2. Import fitz: The main PyMuPDF module, which we‚Äôll use to interact with PDF documents.\n",
        "\n",
        "3. Upload files from our computer to Google Colab.\n"
      ],
      "metadata": {
        "id": "z5WSeL2NyBb6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qj52sXWrEkx"
      },
      "outputs": [],
      "source": [
        "# --- Step 1. Uploading and Reading PDF Files ---\n",
        "# Install necessary library for reading PDF\n",
        "!pip install PyMuPDF  # Install the PyMuPDF library\n",
        "\n",
        "# Import the required library\n",
        "import fitz  # PyMuPDF\n",
        "from google.colab import files # To be able to upload files from our computer\n",
        "\n",
        "# Upload and read our PDF files\n",
        "print('-'*50)\n",
        "print(\"Please upload the three PDF files of School Curricula for Text Extraction and Analysis:\")\n",
        "uploaded_files = files.upload()  # We use the files.upload() function to open\n",
        "                                 # a dialog where we can select and upload our PDF files.\n",
        "                                 # The files.upload() function returns a dictionary-like object.\n",
        "                                 # This dictionary's keys are the filenames of the uploaded files\n",
        "                                 # and the values are the contents of those files.\n",
        "                                 # We assign this object to the variable uploaded_files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 2. Extracting Text from PDF Files\n",
        "\n",
        "In this section, we extract and store the text content of the uploaded PDF files for further analysis.\n",
        "\n",
        "First, we initialize a dictionary.\n",
        "\n",
        "Then we need to extract and store the extracted text.\n",
        "\n",
        "We will then preview the extracted text of the two files to verify successful extraction by printing the first few characters.\n",
        "\n"
      ],
      "metadata": {
        "id": "u63dqxcc2n3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 2. Extracting Text from PDF Files ---\n",
        "# Initialize a dictionary to store the extracted text for each PDF\n",
        "# The keys will be the filenames, and the values will be the corresponding extracted text\n",
        "pdf_texts = {} # this is empty, but will eventually hold the extracted texts\n",
        "pdf_pages = {}\n",
        "\n",
        "# --- TEXT EXTRACTION ---\n",
        "# Extract text from each uploaded PDF\n",
        "for filename in uploaded_files.keys(): # Loop through each uploaded file\n",
        "    pdf_text = \"\" # Inside the loop, pdf_text is reset to an empty string at the beginning of each iteration.\n",
        "    # The line `pdf_pages = \"\"` was removed as pdf_pages should remain a dictionary.\n",
        "\n",
        "    with fitz.open(filename) as doc: # Open the PDF file using PyMuPDF\n",
        "        pdf_pages[filename] = doc.page_count # Store the number of pages for the current PDF\n",
        "        for page in doc: # This inner loop iterates through each page of the currently open PDF document\n",
        "            pdf_text += page.get_text() # This method is used to extract the text from each page and append it\n",
        "    pdf_texts[filename] = pdf_text # After processing all the pages, store the full text of each file in the dictionary, associating it with the file name\n",
        "    print(f\"Text has successfully extracted from {filename}\")\n",
        "\n",
        "'''# --- TEST THE RESULTS ---\n",
        "# Print a summary of the extracted text (first 500 characters for each file)\n",
        "for file_name, text in pdf_texts.items(): # Loop through the dictionary to preview the extracted text\n",
        "   print('='*100)\n",
        "   print(f\"\\nExtracted Text from {file_name} (First 500 characters):\")\n",
        "   print(text[:500])\n",
        "'''\n"
      ],
      "metadata": {
        "id": "KK43uLFA0SRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exported Filename\n",
        "This function checks for specific Greek keywords within the uploaded filenames and assign a corresponding exported_filename for the desired XLS output file."
      ],
      "metadata": {
        "id": "yVeNwgLNoKVm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new-cell-1"
      },
      "source": [
        "def generate_exported_excelfile(filename, all_filenames=None):\n",
        "    # If we have all filenames, check for the cross-level 2023 group\n",
        "    if all_filenames and len(all_filenames) >= 3:\n",
        "        # Check if all files are from 2023 and represent different levels\n",
        "        all_2023 = all('2023' in f for f in all_filenames)\n",
        "        has_demotiko = any(('ŒîŒ∑ŒºŒøœÑ' in f) or ('ŒîŒóŒúŒüŒ§' in f) for f in all_filenames)\n",
        "        has_gymnasio = any(('ŒìœÖŒºŒΩ' in f) or ('ŒìŒ•ŒúŒù' in f) for f in all_filenames)\n",
        "        has_lykeio = any(('ŒõœÖŒ∫' in f) or ('ŒõŒ•Œö' in f) for f in all_filenames)\n",
        "\n",
        "        if all_2023 and has_demotiko and has_gymnasio and has_lykeio:\n",
        "            return 'Œ†Œ£_2023_ŒîŒ∑Œº_ŒìœÖŒºŒΩ_ŒõœÖŒ∫_Text_Metrics.xlsx'\n",
        "\n",
        "    # Original logic for other cases\n",
        "    exported_filename = 'Œ†Œ£_'\n",
        "\n",
        "    # Add level information\n",
        "    if ('ŒîŒ∑ŒºŒøœÑ' in filename) or ('ŒîŒóŒúŒüŒ§' in filename):\n",
        "        exported_filename += 'ŒîŒ∑ŒºŒøœÑŒπŒ∫Œø'\n",
        "    elif ('ŒìœÖŒºŒΩ' in filename) or ('ŒìŒ•ŒúŒù' in filename):\n",
        "        exported_filename += 'ŒìœÖŒºŒΩŒ±œÉŒπŒø'\n",
        "    elif ('ŒõœÖŒ∫' in filename) or ('ŒõŒ•Œö' in filename):\n",
        "        exported_filename += 'ŒõœÖŒ∫ŒµŒπŒø'\n",
        "    elif ('ŒùŒ∑œÄ' in filename) or ('ŒùŒóŒ†' in filename):\n",
        "        exported_filename += 'ŒùŒ∑œÄŒµŒπŒ±Œ≥œâŒ≥ŒµŒπŒø'\n",
        "    else:\n",
        "        exported_filename += 'ŒëŒªŒªŒø'\n",
        "\n",
        "    exported_filename += '_Text_Metrics.xlsx'\n",
        "    return exported_filename\n",
        "\n",
        "# Call this function in the main processing code like this:\n",
        "# excel_filename = generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 3. Calculating Basic Text Metrics\n",
        "\n",
        "The following part of the code defines a function to **calculate basic metrics**: number of characters, number of words, unique words\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pOYg9KL26j4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3. Calculating & Displaying Basic Text Metrics ---\n",
        "# --- CALCULATE\n",
        "# Compare basic metrics between the 3 texts\n",
        "# Define a function to calculate basic text metrics\n",
        "def Calculate_Metrics(text): # we define a function named calculate_metrics\n",
        "                             # that takes a single argument, text, which represents the text to analyze\n",
        "    word_list = text.split()  # Break the text into a list of words using spaces as the delimiter\n",
        "    num_words = len(word_list)  # Total word count\n",
        "    num_chars = len(text)  # Total character count including spaces and punctuation\n",
        "    unique_words = len(set(word_list))  # Unique word count\n",
        "    '''\n",
        "    The set() function creates a set from the word_list.\n",
        "    A set is a data structure that can only contain unique elements.\n",
        "    Any duplicate elements in the original list are removed when you create a set\n",
        "    '''\n",
        "    return num_chars, num_words, unique_words # To be used to output the metrics\n"
      ],
      "metadata": {
        "id": "TwxcXCtmrysV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying and Comparing Text Metrics\n",
        "\n",
        "This part of the code **calculates and displays the key metrics, using the function Calculate_Metrics above, for each uploaded text**.\n"
      ],
      "metadata": {
        "id": "I4Mibhpi8SyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DISPLAY RESULTS\n",
        "# Display metrics for each text\n",
        "print(\"\\nComparing Text Metrics:\")\n",
        "metrics = {} # Creates an empty dictionary to store the calculated metrics for each text.\n",
        "             # Each filename will serve as a key, with its metrics stored as values\n",
        "for filename, text in pdf_texts.items(): # Iterates through the uploaded files and their extracted text\n",
        "    num_chars, num_words, unique_words = Calculate_Metrics(text) # Calls the calculate_metrics function with file_name\n",
        "    metrics[filename] = {\n",
        "        \"Pages\": pdf_pages[filename],\n",
        "        \"Characters\": num_chars,\n",
        "        \"Words\": num_words,\n",
        "        \"Unique Words\": unique_words\n",
        "    } # Saves the calculated metrics for the current file in the metrics dictionary\n",
        "      # Each file is represented by its name as the key and its metrics as a dictionary of values\n",
        "    print(f\"\\nMetrics for {filename}:\") # Display the file name for the metrics to be printed\n",
        "    print(f\"  Number of Pages: {pdf_pages[filename]}\")\n",
        "    print(f\"  Number of Characters: {num_chars}\")\n",
        "    print(f\"  Number of Words: {num_words}\")\n",
        "    print(f\"  Number of Unique Words: {unique_words}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "piNhRvsx6Zdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aTqm91ovo0-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 4. Comparisong of the Text Metrics\n",
        "\n",
        "This part **comparing the Text Matrics calculated before, of the files/texts uploaded**.\n"
      ],
      "metadata": {
        "id": "IR9Jw4rfo1Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4. METRICS COMPARISON ---\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# !pip install xlsxwriter # Install the xlsxwriter library\n",
        "!pip install openpyxl # Install openpyxl for appending sheets to existing Excel files\n",
        "\n",
        "# Compare the 3 texts side by side\n",
        "if len(metrics) == 3:             # Check if there are exactly 3 texts to compare\n",
        "    files = list(metrics.keys())  # Get the list of filenames from the metrics dictionary\n",
        "\n",
        "    # Print a header for the side-by-side comparison\n",
        "    print(\"\\nSide-by-Side Comparison:\")\n",
        "    print(f\"{'Metric':<20} {files[0]:<28} {files[1]:<25} {files[2]:<25}\")  # Print column headers with formatted spacing\n",
        "    print(\"-\" * 99)  # Print a separator line for better readability\n",
        "\n",
        "    # Loop through the keys representing the metrics to display their values\n",
        "    for key in [\"Pages\", \"Characters\", \"Words\", \"Unique Words\"]:\n",
        "        # Print each metric name, and the corresponding values for both texts, side by side\n",
        "        print(f\"{key:<20} {metrics[files[0]][key]:<30} {metrics[files[1]][key]:<25} {metrics[files[2]][key]:<25} \")\n",
        "\n",
        "# --- METRICS PLOTTING ---\n",
        "# Create a DataFrame from the metrics dictionary for plotting\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# --- METRICS STORING ---\n",
        "# Export the metrics DataFrame to an Excel file\n",
        "# Use openpyxl engine with mode='w' to create/overwrite the file initially\n",
        "with pd.ExcelWriter(generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys())), engine='openpyxl', mode='w') as writer:\n",
        "    metrics_df.to_excel(writer, sheet_name='Basic_Metrics', index=True)\n",
        "print(\"\\n Text Metrics also exported œÑŒø\", generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys())), \" \\n\")\n",
        "\n",
        "# Plotting the 2-D column chart\n",
        "metrics_df.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Comparison of Text Metrics across Documents')\n",
        "plt.xlabel('Document')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()\n",
        "\n",
        "\n",
        "'''# Plotting one chart for each metric\n",
        "for metric_name in metrics_df.index: # Iterate through 'Characters', 'Words', 'Unique Words'\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    # Select the row corresponding to the current metric and plot it as a bar chart\n",
        "    metrics_df.loc[metric_name].plot(kind='bar', color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "    plt.title(f'{metric_name} Across Documents')\n",
        "    plt.xlabel('Document')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability\n",
        "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "    plt.show()'''"
      ],
      "metadata": {
        "id": "J_HIYpdl6b3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 5. Clean noise\n",
        "This part cleans the extracted text by removing headers, footers, and page numbers. The text patterns are based on the extracted files."
      ],
      "metadata": {
        "id": "11PC6z4sMRB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5. Clean the noise (unwanted parts) ---\n",
        "\n",
        "import re                           # import re module for Regular Expressions\n",
        "from typing import List, Optional\n",
        "\n",
        "# Function for cleaning the extracted text by removing headers, footers, and page numbers\n",
        "def CleanText(text: str,                                     # the extracted text\n",
        "               header: Optional[List[str]] = None,            # List of header patterns to remove\n",
        "               footer: Optional[List[str]] = None,            # List of footer patterns to remove\n",
        "               page_num: Optional[List[str]] = None) -> str:  # List of regex patterns to identify page numbers\n",
        "\n",
        "    cleaned_text = text\n",
        "\n",
        "    # ŒîŒïŒ†Œ†Œ£2003--NŒì_ŒîŒ∑ŒºŒøœÑŒπŒ∫ŒøœÖ.pdf & Œ†Œ£2011-NŒì_ŒîŒ∑ŒºŒøœÑŒπŒ∫ŒøœÖ.pdf Œ¥ŒµŒΩ Œ≠œáŒøœÖŒΩ Œ∫ŒµŒØŒºŒµŒΩŒø œÉŒµ ŒöŒµœÜŒ±ŒªŒØŒ¥Œ±-Œ•œÄŒøœÉŒ≠ŒªŒπŒ¥Œø.\n",
        "    # Œ§Œø œÄŒ±œÅŒ±Œ∫Œ¨œÑœâ œÉœÖŒΩŒ±ŒΩœÑŒπœéŒΩœÑŒ±Œπ œÉœÑŒ∑ŒΩ ŒöŒµœÜŒ±ŒªŒØŒ¥Œ±-Œ•œÄŒøœÉŒ≠ŒªŒπŒ¥Œø œÑŒøœÖ Œ†Œ£2023-ŒùŒµŒøŒµŒªŒªŒ∑ŒΩŒπŒ∫ŒÆŒìŒªœâœÉœÉŒ±_ŒîHMOTIKOY_Œ†Œ£21v2.pdf\n",
        "    # Customize the header pattern\n",
        "    if header is None:\n",
        "      header = [r'ŒùŒµŒøŒµŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ ŒìŒªœéœÉœÉŒ± ŒîŒ∑ŒºŒøœÑŒπŒ∫Œøœç', r'ŒùŒµŒøŒµŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ ŒìŒªœéœÉœÉŒ± ŒëŒÑ, ŒíŒÑ Œ∫Œ±Œπ ŒìŒÑ ŒìœÖŒºŒΩŒ±œÉŒØŒøœÖ']\n",
        "\n",
        "    # Customize the footer pattern\n",
        "    if footer is None:\n",
        "      footer = [r'ŒïœÄŒπœáŒµŒπœÅŒ∑œÉŒπŒ±Œ∫œå Œ†œÅœåŒ≥œÅŒ±ŒºŒºŒ±', r'ŒïœÄŒπœáŒµŒπœÅŒ∑œÉŒπŒ±Œ∫œå Œ†œÅœåŒ≥œÅŒ±ŒºŒºŒ± ', r'ŒµœÄŒπœáŒµŒπœÅŒ∑œÉŒπŒ±Œ∫œå œÄœÅœåŒ≥œÅŒ±ŒºŒºŒ± ', r'ŒµœÄŒπœáŒµŒπœÅŒ∑œÉŒπŒ±Œ∫œå œÄœÅœåŒ≥œÅŒ±ŒºŒºŒ±.+',\n",
        "                r'ŒëŒΩŒ¨œÄœÑœÖŒæŒ∑ ŒëŒΩŒ∏œÅœéœÄŒπŒΩŒøœÖ ŒîœÖŒΩŒ±ŒºŒπŒ∫Œøœç,', r'ŒëŒΩŒ¨œÄœÑœÖŒæŒ∑ŒëŒΩŒ∏œÅœéœÄŒπŒΩŒøœÖ ŒîœÖŒΩŒ±ŒºŒπŒ∫Œøœç,', r'Œ±ŒΩŒ¨œÄœÑœÖŒæŒ∑ Œ±ŒΩŒ∏œÅœéœÄŒπŒΩŒøœÖ Œ±œÖŒΩŒ±ŒºŒπŒ∫Œøœç,.+',\n",
        "                r'ŒïŒ∫œÄŒ±ŒØŒ¥ŒµœÖœÉŒ∑ Œ∫Œ±Œπ ŒîŒπŒ¨ ŒíŒØŒøœÖ ŒúŒ¨Œ∏Œ∑œÉŒ∑', r'ŒµŒ∫œÄŒ±ŒØŒ¥ŒµœÖœÉŒ∑ Œ∫Œ±Œπ Œ¥ŒπŒ¨ Œ≤ŒØŒøœÖ ŒºŒ¨Œ∏Œ∑œÉŒ∑.+',\n",
        "                r'ŒúŒµ œÑŒ∑ œÉœÖŒ≥œáœÅŒ∑ŒºŒ±œÑŒøŒ¥œåœÑŒ∑œÉŒ∑ œÑŒ∑œÇ ŒïŒªŒªŒ¨Œ¥Œ±œÇ Œ∫Œ±Œπ œÑŒ∑œÇ ŒïœÖœÅœâœÄŒ±œäŒ∫ŒÆœÇ ŒàŒΩœâœÉŒ∑œÇ',r'ŒúŒµ œÑŒ∑ œÉœÖŒ≥œáœÅŒ∑ŒºŒ±œÑŒøŒ¥œåœÑŒ∑œÉŒ∑ œÑŒ∑œÇ ŒïŒªŒªŒ¨Œ¥Œ±œÇ Œ∫Œ±Œπ œÑŒ∑œÇ ŒïœÖœÅœâœÄŒ±œäŒ∫ŒÆœÇ ŒàŒΩœâœÉŒ∑œÇ', r'ŒºŒµ œÑŒ∑ œÉœÖŒ≥œáœÅŒ∑ŒºŒ±œÑŒøŒ¥œåœÑŒ∑œÉŒ∑ œÑŒ∑œÇ ŒµŒªŒªŒ¨Œ¥Œ±œÇ Œ∫Œ±Œπ œÑŒ∑œÇ ŒµœÖœÅœâœÄŒ±œäŒ∫ŒÆœÇ Œ≠ŒΩœâœÉŒ∑œÇ .+',\n",
        "                r'Œ†Œ°ŒüŒìŒ°ŒëŒúŒúŒëŒ§Œë', r'œÄœÅŒøŒ≥œÅŒ¨ŒºŒºŒ±œÑŒ±.+',\n",
        "                r'Œ£Œ†ŒüŒ•ŒîŒ©Œù', r'œÉœÄŒøœÖŒ¥œéŒΩ',\n",
        "                ]\n",
        "\n",
        "    if page_num is None:\n",
        "      page_num = [r\"^\\s*\\d+\\s*$\", r\"^\\s*\\d+\\s*|\", r\"^\\s*\\d+\\s* |\" ]\n",
        "\n",
        "    # Split text into lines for processing\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Skip empty lines\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Check for header patterns\n",
        "        if any(re.fullmatch(pattern, line, re.IGNORECASE) for pattern in header):\n",
        "            continue\n",
        "\n",
        "        # Check for footer patterns\n",
        "        if any(re.fullmatch(pattern, line, re.IGNORECASE) for pattern in footer):\n",
        "            continue\n",
        "\n",
        "        # Check for page number patterns\n",
        "        if any(re.fullmatch(pattern, line) for pattern in page_num):\n",
        "            continue\n",
        "\n",
        "        # If line passed all filters, keep it\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    # Reconstruct the text\n",
        "    cleaned_text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "    # Additional cleanup for remaining artifacts\n",
        "    cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', cleaned_text)  # Reduce multiple newlines\n",
        "    cleaned_text = re.sub(r'[^\\S\\n]{2,}', ' ', cleaned_text)  # Reduce multiple spaces\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "'''# --- TEST RESULTS ---\n",
        "# Test if the repeated text from header and footer are removed\n",
        "for file_name, text in pdf_texts.items(): # Loop through the dictionary to preview the extracted text\n",
        "   text = CleanText(text)\n",
        "   print('='*100)\n",
        "   print(f\"\\nExtracted Text from {file_name} (First 1500 characters):\")\n",
        "   print(text[:10000])\n",
        "'''"
      ],
      "metadata": {
        "id": "hfYiW8IiNA6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 6. Cosine Similarity\n",
        "In this section, we calculate the cosine similarity between the two texts to measure how similar or different they are. First, we convert the text data into numerical vectors using the CountVectorizer. Then, we compute the cosine similarity between these vectors and display the results in a clear table format, where values closer to 1 indicate high similarity, and values closer to 0 indicate more dissimilarity."
      ],
      "metadata": {
        "id": "P7PIUVUscen2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6. COSINE SIMILARITY\n",
        "# Import necessary libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # To convert text data into\n",
        "                                                             # a matrix of token counts\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # To calculate the cosine similarity between vectors\n",
        "import pandas as pd  # For handling data and displaying tables\n",
        "\n",
        "# Create a vector representation of the texts\n",
        "vectorizer = CountVectorizer().fit_transform(pdf_texts.values())  # Convert the texts\n",
        "                                                                  # into a matrix of token counts\n",
        "                                                                  # (bag of words model)\n",
        "\n",
        "vectors = vectorizer.toarray()  # Convert the sparse matrix into a dense array for easier processing\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cos_sim = cosine_similarity(vectors)  # Compute the cosine similarity\n",
        "                                      # between the vectorized text representations\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "cos_sim_df = pd.DataFrame(cos_sim, index=pdf_texts.keys(), columns=pdf_texts.keys())\n",
        "\n",
        "'''\n",
        "Create a DataFrame (table) from the cosine similarity matrix,\n",
        "using the filenames as both row and column labels\n",
        "'''\n",
        "\n",
        "# Explicitly call display() in Google Colab to render the DataFrame as a nicely formatted table\n",
        "from IPython.display import display  # Import display from IPython to render\n",
        "                                     # the DataFrame in a nice table format\n",
        "display(cos_sim_df) # Display the cosine similarity matrix as a well-formatted table in Google Colab\n",
        "\n",
        "# Provide an explanation for the reader of how to interpret the cosine similarity values\n",
        "print(\"\\nThe cosine similarity values range from 0 to 1. A value of 1 indicates that the texts are identical,\")\n",
        "print(\"while a value closer to 0 means the texts are more dissimilar.\")\n",
        "\n",
        "# Save the Cosine_Similarity to an Excel file in a sheet named 'Cosine_Similarity'\n",
        "# Use 'openpyxl' engine and 'a' mode to append to an existing workbook\n",
        "with pd.ExcelWriter(generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys())), engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
        "    cos_sim_df.to_excel(writer, sheet_name='Cosine_Similarity', index=True)\n",
        "print(\"\\n Cosine Similarity also exported œÑŒø\", generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys())), \" \\n\")\n"
      ],
      "metadata": {
        "id": "IdigdpapBTbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 7. Frequencies\n",
        "\n",
        "This code calculates and displays the most frequently occurring words in each text. It uses Python's Counter to count word occurrences and outputs the Top-100 words for each text, along with their frequencies. This provides an overview of the most common terms used in the documents."
      ],
      "metadata": {
        "id": "ffrCy7RsN1CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 7. Calculate words frequency ---\n",
        "\n",
        "from collections import Counter  # Import the Counter class from the collections module\n",
        "import pandas as pd           # Import pandas for DataFrame functionality\n",
        "# !pip install xlsxwriter # already installed in previous section\n",
        "# openpyxl should be installed in the previous step\n",
        "\n",
        "# Define a function to calculate word frequencies\n",
        "def Get_Word_Frequencies(text):\n",
        "    words = text.split()  # Split the text into a list of words based on spaces\n",
        "    return Counter(words)  # Count the frequency of each word in the list and return a Counter object\n",
        "\n",
        "'''# Retrieve and display the 100 most common words along with their frequencies\n",
        "word_freq = Get_Word_Frequencies(text)\n",
        "for word, freq in word_freq.most_common(100):\n",
        "    print(f\"  {word}: {freq}\")  # Print each word and its frequency, formatted for readability'''\n",
        "\n",
        "\n",
        "# Initialize dictionary to store top 100 words and their frequencies for each document\n",
        "top100_words = {}\n",
        "\n",
        "# Iterate over the uploaded PDF texts\n",
        "for filename, text in pdf_texts.items():\n",
        "    word_freq = Get_Word_Frequencies(text)  # Get the word frequencies for the current text\n",
        "    # Get the 100 most common words and their frequencies and store them in a dictionary\n",
        "    top100_words[filename] = dict(word_freq.most_common(100))\n",
        "    # print(f\"\\n--- Top 100 Words in {filename}:\")\n",
        "    # for x, y in top100_words[filename].items():    # Print the Top-100 values in one column, one under the other\n",
        "      # print(x, ':', y)\n",
        "\n",
        "# Create a DataFrame from the collected top words for side-by-side comparison\n",
        "# Fill NaN values with 0, indicating the word was not in the top 100 for that document, and convert to int\n",
        "top100_words_df = pd.DataFrame(top100_words).fillna(0).astype(int)\n",
        "\n",
        "print(\"\\nTop 100 words (preprocessed) Side-by-Side:\")\n",
        "# Display the DataFrame. Using display() for better formatting in Colab if needed.\n",
        "from IPython.display import display\n",
        "display(top100_words_df)\n",
        "\n",
        "# Save the top100_words_df to an Excel file in a sheet named 'Top-100'\n",
        "# Use 'openpyxl' engine and 'a' mode to append to an existing workbook\n",
        "with pd.ExcelWriter(generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys())), engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
        "    top100_words_df.to_excel(writer, sheet_name='Top-100 Pre', index=True)\n",
        "print(\"\\nTop 100 words (preprocessed) have been exported to \", generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys())), \" in a sheet named 'Top-100 Pre'.\")"
      ],
      "metadata": {
        "id": "0nUT8aZTvnD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 8. Frequencies after Processing with Tokenization, POS tagging and Lemmatization\n",
        "The code in the following parts calculates and displays the most frequently occurring words in each text. First tokenization and lemmatization of text takes place and count the Top-25 of Nouns, Adjectives, Adverbs and Verbs occurences. Consequently it count the occurrences of the four POS and outputs the Top-100 words for each text, along with their frequencies. This provides an overview of the most common terms used in the documents."
      ],
      "metadata": {
        "id": "EKMsb0ZkpeMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing - Lowercasing and Stop Words Removal ---\n",
        "\n",
        "# Install NLTK for stop words\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download the stop words list from NLTK\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('greek'))\n",
        "\n",
        "# Add custom stop words\n",
        "custom_stop_words = {\"-\",\"Œ±œÄœå\", \"œÑŒ∑œÇ\", \"œÑŒ∑\", \"œÑŒ∑ŒΩ\", \"œåœÑŒπ\" \"ŒµŒØŒΩŒ±Œπ\" \"œÄœâœÇ\", \"œÑŒø\", \"œÑŒ±\", \"œÉŒµ\", \"œâœÇ\", \"œÑŒøœÖœÇ\", \"œÑŒøœÖ\", \"œÑœâŒΩ\", \"œÑŒπœÇ\", \"ŒµŒØŒΩŒ±Œπ\", \"¬µŒµ\", \"ŒÆ\", \"‚Ä¢\", \"/\", \"‚àí\", \"ÔÇ∑\", \"(œÄ.œá.\", \"Œ≤ŒÑ\", \"Œ≥ŒÑ\", \"Œ≥œÖŒºŒΩŒ±œÉŒØŒøœÖ\", \"œÉ.\", \" ÔÉº:\", \"œÑ.\",\"œÄ.œá.\",\"Œ≤.Œº.\",\"ŒºŒπŒ±\",\"Œ≠ŒΩŒ±\",\"Œ±ŒªŒªŒ¨\",\"œåœÑŒπ\",\"ŒµŒΩœåœÇ\",\"œÑŒøœÖœÇ\",\"Œ∫Œ¨Œ∏Œµ\",\"œÑŒøœÖ/œÑŒ∑œÇ\",\"ŒºŒ≠œÉœâ\",\"Œ±.\",\"œÑŒøœÖœÇ.\",\"œÑŒøœÖœÇ/œÑŒπœÇ\",\"Œ±ŒÑ,\",\"Œ±ŒÑ\",\"ÔÄ≠\",\"ÔÉº \",\"ÔÉº\",\"/\",\"|\", \"œÉœÑŒ±\", \"ŒºŒπŒ±œÇ\", \"œÉœÑŒπœÇ\", \"‚Äì\", \"ŒΩŒ±:\", \"¬µŒπŒ±\", \"&\", \"1\", \"1.\", \"2\", \"3.\", \"Œ≤.Œº.\", \"Œ∫œÑŒª.\", \"¬µŒπŒ±\", \"Œ±-Œ≤\", \"Œª.œá.\"  }  # Add stop words here\n",
        "stop_words.update(custom_stop_words)  # Update the stop words set with the custom words\n",
        "\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Split text into words and remove stop words\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words] # Iterate through each word in the words list.\n",
        "                                                                        # In each iteration, the current word is assigned to the variable word.\n",
        "    return filtered_words"
      ],
      "metadata": {
        "id": "I7_GBWm99GVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results\n",
        "The following function stores the results in the exported Excel file in 2 new sheets, titled 'Word_Frequencies' & 'Top-100_POS'"
      ],
      "metadata": {
        "id": "aA8McEvb7q-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores the frequency data to Excel with the specified organization\n",
        "# Appends new sheets to existing Excel file - Colab compatible version\n",
        "def Store_Data_toExcel(top25_data_by_doc, top100_data_by_doc, summary_data_by_doc, excelfile):\n",
        "    try:\n",
        "        # Check if file exists in Colab environment\n",
        "        if os.path.exists(excelfile):\n",
        "            # Load existing workbook\n",
        "            wb = openpyxl.load_workbook(excelfile)\n",
        "            print(f\"‚úì Appending to existing Excel file: '{excelfile}'\")\n",
        "        else:\n",
        "            # Create new workbook if it doesn't exist\n",
        "            wb = Workbook()\n",
        "            # Remove default sheet\n",
        "            wb.remove(wb['Sheet'])\n",
        "            print(f\"‚úì Creating new Excel file: '{excelfile}'\")\n",
        "\n",
        "        # --- SHEET 1: Summary_POS (POS Summary for each document) ---\n",
        "        # Remove existing sheet if it exists\n",
        "        if 'Summary_POS' in wb.sheetnames:\n",
        "            del wb['Summary_POS']\n",
        "        ws1 = wb.create_sheet(\"Summary_POS\")\n",
        "\n",
        "        # Define starting columns for each document\n",
        "        doc_columns = {\n",
        "            0: 'A',  # Document 1\n",
        "            1: 'F',  # Document 2\n",
        "            2: 'K'   # Document 3\n",
        "        }\n",
        "\n",
        "        # Populate Sheet 1 - Summary_POS\n",
        "        for doc_idx, (doc_filename, summary_data) in enumerate(summary_data_by_doc.items()):\n",
        "            start_col = doc_columns.get(doc_idx, 'A')\n",
        "\n",
        "            # Document title\n",
        "            ws1[f'{start_col}1'] = doc_filename\n",
        "\n",
        "            # Headers for summary\n",
        "            ws1[f'{start_col}2'] = 'POS_Category'\n",
        "            ws1[f'{chr(ord(start_col)+1)}2'] = 'Unique_Words'\n",
        "            ws1[f'{chr(ord(start_col)+2)}2'] = 'Total_Words'\n",
        "\n",
        "            current_row = 3\n",
        "\n",
        "            # Add summary data for each POS category\n",
        "            pos_categories = ['NOUNS', 'ADJECTIVES', 'ADVERBS', 'VERBS', 'TOTAL_POS']\n",
        "            for pos_cat in pos_categories:\n",
        "                if pos_cat in summary_data:\n",
        "                    ws1[f'{start_col}{current_row}'] = pos_cat\n",
        "                    ws1[f'{chr(ord(start_col)+1)}{current_row}'] = summary_data[pos_cat]['unique']\n",
        "                    ws1[f'{chr(ord(start_col)+2)}{current_row}'] = summary_data[pos_cat]['total']\n",
        "                    current_row += 1\n",
        "\n",
        "        # --- SHEET 2: Word_Frequencies (Top-25 by POS) ---\n",
        "        # Remove existing sheet if it exists\n",
        "        if 'Top-25_POS' in wb.sheetnames:\n",
        "            del wb['Top-25_POS']\n",
        "        ws2 = wb.create_sheet(\"Top-25_POS\")\n",
        "\n",
        "        # Define starting columns for each document\n",
        "        doc_columns_sheet2 = {\n",
        "            0: 'A',  # Document 1\n",
        "            1: 'F',  # Document 2\n",
        "            2: 'K'   # Document 3\n",
        "        }\n",
        "\n",
        "        # Populate Sheet 2\n",
        "        for doc_idx, (doc_filename, pos_data) in enumerate(top25_data_by_doc.items()):\n",
        "            start_col = doc_columns_sheet2.get(doc_idx, 'A')\n",
        "\n",
        "            # Document title\n",
        "            ws2[f'{start_col}1'] = doc_filename\n",
        "\n",
        "            # Headers\n",
        "            ws2[f'{start_col}2'] = 'POS_category'\n",
        "            ws2[f'{chr(ord(start_col)+1)}2'] = 'Rank'\n",
        "            ws2[f'{chr(ord(start_col)+2)}2'] = 'Word'\n",
        "            ws2[f'{chr(ord(start_col)+3)}2'] = 'Frequency'\n",
        "\n",
        "            current_row = 3\n",
        "\n",
        "            # Add data for each POS category\n",
        "            for pos_category, data in pos_data.items():\n",
        "                for rank, (word, freq) in data:\n",
        "                    ws2[f'{start_col}{current_row}'] = pos_category\n",
        "                    ws2[f'{chr(ord(start_col)+1)}{current_row}'] = rank\n",
        "                    ws2[f'{chr(ord(start_col)+2)}{current_row}'] = word\n",
        "                    ws2[f'{chr(ord(start_col)+3)}{current_row}'] = freq\n",
        "                    current_row += 1\n",
        "                current_row += 1  # Add empty row between POS categories\n",
        "\n",
        "        # --- SHEET 3: Top-100_POS ---\n",
        "        # Remove existing sheet if it exists\n",
        "        if 'Top-100_POS' in wb.sheetnames:\n",
        "            del wb['Top-100_POS']\n",
        "        ws3 = wb.create_sheet(\"Top-100_POS\")\n",
        "\n",
        "        # Define starting columns for each document in sheet 3\n",
        "        doc_columns_sheet3 = {\n",
        "            0: 'B',  # Document 1\n",
        "            1: 'E',  # Document 2\n",
        "            2: 'H'   # Document 3\n",
        "        }\n",
        "\n",
        "        # Populate Sheet 3\n",
        "        for doc_idx, (doc_filename, top100_data) in enumerate(top100_data_by_doc.items()):\n",
        "            start_col = doc_columns_sheet3.get(doc_idx, 'B')\n",
        "\n",
        "            # Document title\n",
        "            ws3[f'{start_col}1'] = doc_filename\n",
        "\n",
        "            # Headers - Adjusted to match specification\n",
        "            if doc_idx == 0:  # Document 1\n",
        "                ws3['A2'] = 'Rank'\n",
        "                ws3['B2'] = 'Word'\n",
        "                ws3['C2'] = 'COUNT'\n",
        "            elif doc_idx == 1:  # Document 2\n",
        "                ws3['E2'] = 'Word'\n",
        "                ws3['F2'] = 'COUNT'\n",
        "            elif doc_idx == 2:  # Document 3\n",
        "                ws3['H2'] = 'Word'\n",
        "                ws3['I2'] = 'COUNT'\n",
        "\n",
        "            current_row = 3\n",
        "\n",
        "            # Add Top-100 data\n",
        "            for rank, (word, freq) in top100_data:\n",
        "                if doc_idx == 0:  # Document 1\n",
        "                    ws3[f'A{current_row}'] = rank\n",
        "                    ws3[f'B{current_row}'] = word\n",
        "                    ws3[f'C{current_row}'] = freq\n",
        "                elif doc_idx == 1:  # Document 2\n",
        "                    ws3[f'E{current_row}'] = word\n",
        "                    ws3[f'F{current_row}'] = freq\n",
        "                elif doc_idx == 2:  # Document 3\n",
        "                    ws3[f'H{current_row}'] = word\n",
        "                    ws3[f'I{current_row}'] = freq\n",
        "\n",
        "                current_row += 1\n",
        "\n",
        "        # Save the workbook\n",
        "        wb.save(excelfile)\n",
        "        print(f\"‚úì Successfully updated Excel file: '{excelfile}'\")\n",
        "        print(\"‚úì Sheet 'Summary_POS': POS summary statistics by document\")\n",
        "        print(\"‚úì Sheet 'Word_Frequencies': Top-25 for each POS category by document\")\n",
        "        print(\"‚úì Sheet 'Top-100_POS': Top-100 words across all POS by document\")\n",
        "\n",
        "        # Offer download in Colab\n",
        "        # print(f\"\\nüì• Download the Excel file:\")\n",
        "        # files.download(excelfile)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error updating Excel file: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "wDAyyFv2R1rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculates the Frequencies\n",
        "This part calculates and displays the most frequently occurring words in each text: the Top-25 of Nouns, Adjectives, Adverbs and Verbs occurences and also the occurrences of the four POS and outputs the Top-100 words for each text, along with their frequencies. The calculations made after tokenization and lemmatization of text."
      ],
      "metadata": {
        "id": "63rPwg8P-U-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8. Frequencies after Processing ---  (Not saving POS Summary)\n",
        "# Install spaCy and Greek language model\n",
        "!pip install spacy\n",
        "!pip install openpyxl\n",
        "!pip install matplotlib\n",
        "!python -m spacy download el_core_news_sm  # Greek language model\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "import os\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the Greek model\n",
        "try:\n",
        "    nlp = spacy.load(\"el_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Greek model not found. Downloading...\")\n",
        "    !python -m spacy download el_core_news_sm\n",
        "    nlp = spacy.load(\"el_core_news_sm\")\n",
        "\n",
        "# Initialize counters\n",
        "noun_counts = Counter()\n",
        "adj_counts = Counter()\n",
        "adv_counts = Counter()\n",
        "verb_counts = Counter()\n",
        "all_pos_counts = Counter()\n",
        "\n",
        "# Lists to store data for all sheets\n",
        "top25_data_by_doc = {}  # {doc_name: {pos_type: [(rank, word, freq)]}}\n",
        "top100_data_by_doc = {}  # {doc_name: [(rank, word, freq)]}\n",
        "summary_data_by_doc = {}  # {doc_name: {category: {'unique': x, 'total': y}}}\n",
        "# Lists to store Top-25 and Top-100 results for comparison\n",
        "top25_nouns = {}\n",
        "top25_adj = {}\n",
        "top25_adv = {}\n",
        "top25_verbs = {}\n",
        "top100_allpos = {}\n",
        "\n",
        "# Lists to store summary data for plotting\n",
        "plot_summary_data = {\n",
        "    'filenames': [],\n",
        "    'nouns_total': [],\n",
        "    'adjectives_total': [],\n",
        "    'adverbs_total': [],\n",
        "    'verbs_total': [],\n",
        "    'total_pos': []\n",
        "}\n",
        "\n",
        "# Process each text\n",
        "doc_counter = 0\n",
        "for filename, text in pdf_texts.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Analyzing: {filename}\")\n",
        "    print(f\"{'-'*60}\")\n",
        "\n",
        "    # Add error handling for text processing\n",
        "    try:\n",
        "        text = CleanText(text)  # Clean the text from noise of header and footer\n",
        "        filtered_words = preprocess_text(text)\n",
        "        processed_text = \" \".join(filtered_words)\n",
        "\n",
        "        # Process in chunks if text is too long (Colab memory optimization)\n",
        "        if len(processed_text) > 1000000:  # If text is > 1MB\n",
        "            print(\"‚ö† Large text detected, processing in chunks...\")\n",
        "            doc = nlp(processed_text[:1000000])  # Process first 1MB\n",
        "        else:\n",
        "            doc = nlp(processed_text)\n",
        "\n",
        "        # Reset counters for each document\n",
        "        doc_noun_counts = Counter()\n",
        "        doc_adj_counts = Counter()\n",
        "        doc_adv_counts = Counter()\n",
        "        doc_verb_counts = Counter()\n",
        "        doc_all_pos_counts = Counter()\n",
        "\n",
        "        # Iterate through tokens in the document with progress indicator\n",
        "        total_tokens = len(doc)\n",
        "        print(f\"Processing {total_tokens} tokens...\")\n",
        "\n",
        "        for i, token in enumerate(doc):\n",
        "            # Progress indicator for large documents\n",
        "            if i % 10000 == 0 and i > 0:\n",
        "                print(f\"Processed {i}/{total_tokens} tokens...\")\n",
        "\n",
        "            lemma = token.lemma_.lower().strip()\n",
        "            pos = token.pos_\n",
        "\n",
        "            if not lemma or token.is_punct or token.is_space:\n",
        "                continue\n",
        "\n",
        "            if pos == \"NOUN\":\n",
        "                doc_noun_counts[lemma] += 1\n",
        "            elif pos == \"ADJ\":\n",
        "                doc_adj_counts[lemma] += 1\n",
        "            elif pos == \"ADV\":\n",
        "                doc_adv_counts[lemma] += 1\n",
        "            elif pos == \"VERB\":\n",
        "                doc_verb_counts[lemma] += 1\n",
        "\n",
        "            if pos in [\"NOUN\", \"ADJ\", \"ADV\", \"VERB\"]:\n",
        "                doc_all_pos_counts[lemma] += 1\n",
        "\n",
        "        # Print results with summary\n",
        "        print(f\"\\nüìä SUMMARY FOR {filename}:\")\n",
        "        print(f\"  Nouns: {len(doc_noun_counts)} unique\")\n",
        "        print(f\"  Adjectives: {len(doc_adj_counts)} unique\")\n",
        "        print(f\"  Adverbs: {len(doc_adv_counts)} unique\")\n",
        "        print(f\"  Verbs: {len(doc_verb_counts)} unique\")\n",
        "        print(f\"  Total POS words: {sum(doc_all_pos_counts.values())}\")\n",
        "\n",
        "        # Store data for plotting\n",
        "        plot_summary_data['filenames'].append(filename)\n",
        "        plot_summary_data['nouns_total'].append(sum(doc_noun_counts.values()))\n",
        "        plot_summary_data['adjectives_total'].append(sum(doc_adj_counts.values()))\n",
        "        plot_summary_data['adverbs_total'].append(sum(doc_adv_counts.values()))\n",
        "        plot_summary_data['verbs_total'].append(sum(doc_verb_counts.values()))\n",
        "        plot_summary_data['total_pos'].append(sum(doc_all_pos_counts.values()))\n",
        "\n",
        "        # Store Top-25 Nouns in list\n",
        "        top25_nouns[filename] = doc_noun_counts.most_common(25)\n",
        "        print(f\"\\n--- TOP 25 NOUNS in {filename} ---\")\n",
        "        for word, count in doc_noun_counts.most_common(25):\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "        # Store Top-25 Adjectives in list\n",
        "        top25_adj[filename] = doc_adj_counts.most_common(25)\n",
        "        print(f\"\\n--- TOP 25 ADJECTIVES in {filename} ---\")\n",
        "        for word, count in doc_adj_counts.most_common(25):\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "         # Store Top-25 Adverbs in list\n",
        "        top25_adv[filename] = doc_adv_counts.most_common(25)\n",
        "        print(f\"\\n--- TOP 25 ADVERBS in {filename} ---\")\n",
        "        for word, count in doc_adv_counts.most_common(25):\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "        # Store Top-25 Verbs in list\n",
        "        top25_verbs[filename] = doc_verb_counts.most_common(25)\n",
        "        print(f\"\\n--- TOP 25 VERBS in {filename} ---\")\n",
        "        for word, count in doc_verb_counts.most_common(25):\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "        # Store Top-100 All POS in list\n",
        "        top100_allpos[filename] = doc_all_pos_counts.most_common(100)\n",
        "        print(f\"\\n--- TOP 100 All POS in {filename} ---\")\n",
        "        for word, count in doc_all_pos_counts.most_common(100):\n",
        "            print(f\"{word}: {count}\")\n",
        "\n",
        "        # Update global counters\n",
        "        noun_counts.update(doc_noun_counts)\n",
        "        adj_counts.update(doc_adj_counts)\n",
        "        adv_counts.update(doc_adv_counts)\n",
        "        verb_counts.update(doc_verb_counts)\n",
        "        all_pos_counts.update(doc_all_pos_counts)\n",
        "\n",
        "        # --- STORE DATA FOR EXCEL ---\n",
        "\n",
        "        # Store Top-25 data organized by POS category\n",
        "        top25_data_by_doc[filename] = {\n",
        "            'NOUN': list(enumerate(doc_noun_counts.most_common(25), 1)),\n",
        "            'ADJECTIVE': list(enumerate(doc_adj_counts.most_common(25), 1)),\n",
        "            'ADVERB': list(enumerate(doc_adv_counts.most_common(25), 1)),\n",
        "            'VERB': list(enumerate(doc_verb_counts.most_common(25), 1))\n",
        "        }\n",
        "\n",
        "        # Store Top-100 data (all POS combined)\n",
        "        top100_data_by_doc[filename] = list(enumerate(doc_all_pos_counts.most_common(100), 1))\n",
        "\n",
        "        # Store Summary data for the new Summary_POS sheet\n",
        "        summary_data_by_doc[filename] = {\n",
        "            'NOUNS': {'unique': len(doc_noun_counts), 'total': sum(doc_noun_counts.values())},\n",
        "            'ADJECTIVES': {'unique': len(doc_adj_counts), 'total': sum(doc_adj_counts.values())},\n",
        "            'ADVERBS': {'unique': len(doc_adv_counts), 'total': sum(doc_adv_counts.values())},\n",
        "            'VERBS': {'unique': len(doc_verb_counts), 'total': sum(doc_verb_counts.values())},\n",
        "            'TOTAL_POS': {'unique': len(doc_all_pos_counts), 'total': sum(doc_all_pos_counts.values())}\n",
        "        }\n",
        "\n",
        "        doc_counter += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {filename}: {e}\")\n",
        "        continue\n",
        "\n",
        "# --- CREATE SUMMARY PLOT ---\n",
        "def create_summary_plot(plot_data):\n",
        "    \"\"\"\n",
        "    Create a grouped bar plot showing POS summary by file\n",
        "    \"\"\"\n",
        "    filenames = plot_data['filenames']\n",
        "    n_files = len(filenames)\n",
        "\n",
        "    # Set up the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Define the categories and their colors\n",
        "    categories = ['Nouns', 'Adjectives', 'Adverbs', 'Verbs', 'Total POS']\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "\n",
        "    # Set the width of bars and positions\n",
        "    bar_width = 0.15\n",
        "    x_pos = np.arange(n_files)\n",
        "\n",
        "    # Create bars for each category\n",
        "    for i, (category, color) in enumerate(zip(categories, colors)):\n",
        "        if category == 'Nouns':\n",
        "            values = plot_data['nouns_total']\n",
        "        elif category == 'Adjectives':\n",
        "            values = plot_data['adjectives_total']\n",
        "        elif category == 'Adverbs':\n",
        "            values = plot_data['adverbs_total']\n",
        "        elif category == 'Verbs':\n",
        "            values = plot_data['verbs_total']\n",
        "        else:  # Total POS\n",
        "            values = plot_data['total_pos']\n",
        "\n",
        "        positions = x_pos + (i - 2) * bar_width\n",
        "        bars = ax.bar(positions, values, bar_width, label=category, color=color, alpha=0.8)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                   f'{value:,}', ha='center', va='bottom', fontsize=9, rotation=0)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_xlabel('Documents', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Word Count', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('POS Distribution Summary by Document', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x_pos)\n",
        "\n",
        "    # Shorten filenames for better display\n",
        "    short_names = [name[:20] + '...' if len(name) > 20 else name for name in filenames]\n",
        "    ax.set_xticklabels(short_names, rotation=45, ha='right')\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Add grid for better readability\n",
        "    ax.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Generate and display the plot\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"GENERATING SUMMARY PLOT\")\n",
        "print(f\"{'-'*60}\")\n",
        "\n",
        "if plot_summary_data['filenames']:\n",
        "    summary_plot = create_summary_plot(plot_summary_data)\n",
        "    print(\"‚úÖ Summary plot generated successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå No data available for plotting\")\n",
        "\n",
        "# --- CALL THE FUNCTION TO STORE DATA TO EXCEL ---\n",
        "if top25_data_by_doc and top100_data_by_doc and summary_data_by_doc:\n",
        "    # Use the generated filename from your existing function\n",
        "    excel_file = generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys())) if pdf_texts else 'Œ†Œ£_ŒëŒªŒªŒø_Metrics.xlsx'\n",
        "    Store_Data_toExcel(top25_data_by_doc, top100_data_by_doc, summary_data_by_doc, excel_file)\n",
        "else:\n",
        "    print(\"‚ùå No data to save to Excel\")\n",
        "\n",
        "# Print overall statistics across all documents\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"OVERALL STATISTICS ACROSS ALL TEXTS\")\n",
        "print(f\"{'-'*60}\")\n",
        "\n",
        "print(f\"\\n--- OVERALL TOP 100 WORDS (All POS) ---\")\n",
        "for word, count in all_pos_counts.most_common(100):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\n--- OVERALL TOP 25 NOUNS ---\")\n",
        "for word, count in noun_counts.most_common(25):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\n--- OVERALL TOP 25 ADJECTIVES ---\")\n",
        "for word, count in adj_counts.most_common(25):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\n--- OVERALL TOP 25 ADVERBS ---\")\n",
        "for word, count in adv_counts.most_common(25):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\n--- OVERALL TOP 25 VERBS ---\")\n",
        "for word, count in verb_counts.most_common(25):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Print POS distribution summary\n",
        "print(f\"\\n--- POS DISTRIBUTION SUMMARY ---\")\n",
        "print(f\"Total Nouns: {sum(noun_counts.values())} (Unique: {len(noun_counts)})\")\n",
        "print(f\"Total Adjectives: {sum(adj_counts.values())} (Unique: {len(adj_counts)})\")\n",
        "print(f\"Total Adverbs: {sum(adv_counts.values())} (Unique: {len(adv_counts)})\")\n",
        "print(f\"Total Verbs: {sum(verb_counts.values())} (Unique: {len(verb_counts)})\")\n",
        "print(f\"Total Words Counted: {sum(all_pos_counts.values())} (Unique: {len(all_pos_counts)})\")"
      ],
      "metadata": {
        "id": "mAa1OPidhpJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 9. Word Frequencies Comparisons\n",
        "The follwoing code provides comprehensive comparisons for all POS categories and percentage calculations for new words in the 3rd document. Also saves results to a new \"POS_Comparisons\" sheet in the existing Excel file.  First the following function implements the comparisons:\n",
        "* Top-25 Nouns, Adjectives, Adverbs, Verbs comparisons\n",
        "* All POS Top-100 comparisons\n",
        "* Overlap analysis between documents\n",
        "* New word identification in 3rd document\n",
        "* Percentage calculations for novelty"
      ],
      "metadata": {
        "id": "55xbQOsClNws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "import os\n",
        "\n",
        "# Compare POS lists across documents\n",
        "def compare_pos_lists(top25_nouns, top25_adj, top25_adv, top25_verbs, top100_allpos, excel_file):\n",
        "\n",
        "    # Get filenames\n",
        "    filenames = list(top25_nouns.keys())\n",
        "    if len(filenames) < 3:\n",
        "        print(\"‚ùå Need at least 3 documents for comparison\")\n",
        "        return\n",
        "\n",
        "    filename1, filename2, filename3 = filenames[0], filenames[1], filenames[2]\n",
        "\n",
        "    # Extract just the words from the (word, frequency) tuples\n",
        "    nouns1 = set([word for word, freq in top25_nouns[filename1]])\n",
        "    nouns2 = set([word for word, freq in top25_nouns[filename2]])\n",
        "    nouns3 = set([word for word, freq in top25_nouns[filename3]])\n",
        "\n",
        "    adj1 = set([word for word, freq in top25_adj[filename1]])\n",
        "    adj2 = set([word for word, freq in top25_adj[filename2]])\n",
        "    adj3 = set([word for word, freq in top25_adj[filename3]])\n",
        "\n",
        "    adv1 = set([word for word, freq in top25_adv[filename1]])\n",
        "    adv2 = set([word for word, freq in top25_adv[filename2]])\n",
        "    adv3 = set([word for word, freq in top25_adv[filename3]])\n",
        "\n",
        "    verbs1 = set([word for word, freq in top25_verbs[filename1]])\n",
        "    verbs2 = set([word for word, freq in top25_verbs[filename2]])\n",
        "    verbs3 = set([word for word, freq in top25_verbs[filename3]])\n",
        "\n",
        "    allpos1 = set([word for word, freq in top100_allpos[filename1]])\n",
        "    allpos2 = set([word for word, freq in top100_allpos[filename2]])\n",
        "    allpos3 = set([word for word, freq in top100_allpos[filename3]])\n",
        "\n",
        "    # Calculate comparisons\n",
        "    comparison_results = {}\n",
        "\n",
        "    # NOUNS comparisons\n",
        "    comparison_results['NOUNS'] = {\n",
        "        '1a_nouns_1_in_2': len(nouns1.intersection(nouns2)),\n",
        "        '1b_nouns_2_in_3': len(nouns2.intersection(nouns3)),\n",
        "        '1c_new_nouns_in_3': len(nouns3 - nouns2 - nouns1),\n",
        "        '1c_percentage_new_nouns': len(nouns3 - nouns2 - nouns1) / len(nouns3) * 100 if nouns3 else 0\n",
        "    }\n",
        "\n",
        "    # ADJECTIVES comparisons\n",
        "    comparison_results['ADJECTIVES'] = {\n",
        "        '1a_adj_1_in_2': len(adj1.intersection(adj2)),\n",
        "        '1b_adj_2_in_3': len(adj2.intersection(adj3)),\n",
        "        '1c_new_adj_in_3': len(adj3 - adj2 - adj1),\n",
        "        '1c_percentage_new_adj': len(adj3 - adj2 - adj1) / len(adj3) * 100 if adj3 else 0\n",
        "    }\n",
        "\n",
        "    # ADVERBS comparisons\n",
        "    comparison_results['ADVERBS'] = {\n",
        "        '1a_adv_1_in_2': len(adv1.intersection(adv2)),\n",
        "        '1b_adv_2_in_3': len(adv2.intersection(adv3)),\n",
        "        '1c_new_adv_in_3': len(adv3 - adv2 - adv1),\n",
        "        '1c_percentage_new_adv': len(adv3 - adv2 - adv1) / len(adv3) * 100 if adv3 else 0\n",
        "    }\n",
        "\n",
        "    # VERBS comparisons\n",
        "    comparison_results['VERBS'] = {\n",
        "        '1a_verbs_1_in_2': len(verbs1.intersection(verbs2)),\n",
        "        '1b_verbs_2_in_3': len(verbs2.intersection(verbs3)),\n",
        "        '1c_new_verbs_in_3': len(verbs3 - verbs2 - verbs1),\n",
        "        '1c_percentage_new_verbs': len(verbs3 - verbs2 - verbs1) / len(verbs3) * 100 if verbs3 else 0\n",
        "    }\n",
        "\n",
        "    # TOP-100 ALL POS comparisons\n",
        "    comparison_results['TOP100_ALL_POS'] = {\n",
        "        '1a_allpos_1_in_2': len(allpos1.intersection(allpos2)),\n",
        "        '1b_allpos_2_in_3': len(allpos2.intersection(allpos3)),\n",
        "        '1c_new_allpos_in_3': len(allpos3 - allpos2 - allpos1),\n",
        "        '1c_percentage_new_allpos': len(allpos3 - allpos2 - allpos1) / len(allpos3) * 100 if allpos3 else 0\n",
        "    }\n",
        "\n",
        "    # Print results by GROUP\n",
        "    print(\"COMPARISON RESULTS\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    print(f\"\\nüìä COMPARISON BETWEEN:\")\n",
        "    print(f\"  Document 1: {filename1}\")\n",
        "    print(f\"  Document 2: {filename2}\")\n",
        "    print(f\"  Document 3: {filename3}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    # NOUNS Group\n",
        "    print(f\"\\nüî§ NOUNS COMPARISON:\")\n",
        "    print(f\"  1a. Nouns from {filename1} also in {filename2}: {comparison_results['NOUNS']['1a_nouns_1_in_2']}/25\")\n",
        "    print(f\"  1b. Nouns from {filename2} also in {filename3}: {comparison_results['NOUNS']['1b_nouns_2_in_3']}/25\")\n",
        "    print(f\"  1c. NEW nouns in {filename3}: {comparison_results['NOUNS']['1c_new_nouns_in_3']}/25 ({comparison_results['NOUNS']['1c_percentage_new_nouns']:.1f}%)\")\n",
        "\n",
        "    # ADJECTIVES Group\n",
        "    print(f\"\\nüé® ADJECTIVES COMPARISON:\")\n",
        "    print(f\"  1a. Adjectives from {filename1} also in {filename2}: {comparison_results['ADJECTIVES']['1a_adj_1_in_2']}/25\")\n",
        "    print(f\"  1b. Adjectives from {filename2} also in {filename3}: {comparison_results['ADJECTIVES']['1b_adj_2_in_3']}/25\")\n",
        "    print(f\"  1c. NEW adjectives in {filename3}: {comparison_results['ADJECTIVES']['1c_new_adj_in_3']}/25 ({comparison_results['ADJECTIVES']['1c_percentage_new_adj']:.1f}%)\")\n",
        "\n",
        "    # ADVERBS Group\n",
        "    print(f\"\\n‚ö° ADVERBS COMPARISON:\")\n",
        "    print(f\"  1a. Adverbs from {filename1} also in {filename2}: {comparison_results['ADVERBS']['1a_adv_1_in_2']}/25\")\n",
        "    print(f\"  1b. Adverbs from {filename2} also in {filename3}: {comparison_results['ADVERBS']['1b_adv_2_in_3']}/25\")\n",
        "    print(f\"  1c. NEW adverbs in {filename3}: {comparison_results['ADVERBS']['1c_new_adv_in_3']}/25 ({comparison_results['ADVERBS']['1c_percentage_new_adv']:.1f}%)\")\n",
        "\n",
        "    # VERBS Group\n",
        "    print(f\"\\nüé≠ VERBS COMPARISON:\")\n",
        "    print(f\"  1a. Verbs from {filename1} also in {filename2}: {comparison_results['VERBS']['1a_verbs_1_in_2']}/25\")\n",
        "    print(f\"  1b. Verbs from {filename2} also in {filename3}: {comparison_results['VERBS']['1b_verbs_2_in_3']}/25\")\n",
        "    print(f\"  1c. NEW verbs in {filename3}: {comparison_results['VERBS']['1c_new_verbs_in_3']}/25 ({comparison_results['VERBS']['1c_percentage_new_verbs']:.1f}%)\")\n",
        "\n",
        "    # TOP-100 ALL POS Group\n",
        "    print(f\"\\nüìà TOP-100 ALL POS COMPARISON:\")\n",
        "    print(f\"  1a. Words from {filename1} also in {filename2}: {comparison_results['TOP100_ALL_POS']['1a_allpos_1_in_2']}/100\")\n",
        "    print(f\"  1b. Words from {filename2} also in {filename3}: {comparison_results['TOP100_ALL_POS']['1b_allpos_2_in_3']}/100\")\n",
        "    print(f\"  1c. NEW words in {filename3}: {comparison_results['TOP100_ALL_POS']['1c_new_allpos_in_3']}/100 ({comparison_results['TOP100_ALL_POS']['1c_percentage_new_allpos']:.1f}%)\")\n",
        "\n",
        "    # Save to Excel\n",
        "    save_comparison_to_excel(comparison_results, filenames, excel_file)\n",
        "\n",
        "    return comparison_results"
      ],
      "metadata": {
        "id": "kewod1g0mRoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results\n",
        "the following function provides the Excel integration and saves results to a new \"POS_Comparisons\" sheet in the existing Excel file."
      ],
      "metadata": {
        "id": "l0eCFaKvoy8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save comparison results to Excel file\n",
        "def save_comparison_to_excel(comparison_results, filenames, excel_file):\n",
        "\n",
        "    try:\n",
        "        # Check if file exists\n",
        "        if os.path.exists(excel_file):\n",
        "            wb = openpyxl.load_workbook(excel_file)\n",
        "            print(f\"\\n‚úì Appending to existing Excel file: '{excel_file}'\")\n",
        "        else:\n",
        "            wb = Workbook()\n",
        "            # Remove default sheet\n",
        "            wb.remove(wb['Sheet'])\n",
        "            print(f\"‚úì Creating new Excel file: '{excel_file}'\")\n",
        "\n",
        "        # Remove existing comparison sheet if it exists\n",
        "        if 'POS_Comparisons' in wb.sheetnames:\n",
        "            del wb['POS_Comparisons']\n",
        "\n",
        "        # Create new sheet for comparisons\n",
        "        ws = wb.create_sheet(\"POS_Comparisons\")\n",
        "\n",
        "        # Add title and document info\n",
        "        ws['A1'] = \"POS CATEGORY COMPARISONS\"\n",
        "        ws['E1'] = f\"Comparing: {filenames[0]}, {filenames[1]}, {filenames[2]}\"\n",
        "\n",
        "        # Headers\n",
        "        headers = ['POS Category', 'AA', 'Count', 'Percentage', 'Description']\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            ws.cell(row=4, column=col, value=header)\n",
        "\n",
        "        current_row = 5\n",
        "\n",
        "        # Define comparison descriptions\n",
        "        descriptions = {\n",
        "            '1a': f\"Words from {filenames[0]} also in {filenames[1]}\",\n",
        "            '1b': f\"Words from {filenames[1]} also in {filenames[2]}\",\n",
        "            '1c': f\"NEW words in {filenames[2]}\"\n",
        "        }\n",
        "\n",
        "        # Add data for each POS category\n",
        "        for pos_category, results in comparison_results.items():\n",
        "            # Add category header\n",
        "            ws.cell(row=current_row, column=1, value=pos_category)\n",
        "            current_row += 1\n",
        "\n",
        "            # Add comparison results\n",
        "            comparisons = [\n",
        "                ('1a', results.get('1a_nouns_1_in_2', results.get('1a_adj_1_in_2', results.get('1a_adv_1_in_2', results.get('1a_verbs_1_in_2', results.get('1a_allpos_1_in_2')))))),\n",
        "                ('1b', results.get('1b_nouns_2_in_3', results.get('1b_adj_2_in_3', results.get('1b_adv_2_in_3', results.get('1b_verbs_2_in_3', results.get('1b_allpos_2_in_3')))))),\n",
        "                ('1c', results.get('1c_new_nouns_in_3', results.get('1c_new_adj_in_3', results.get('1c_new_adv_in_3', results.get('1c_new_verbs_in_3', results.get('1c_new_allpos_in_3'))))))\n",
        "            ]\n",
        "\n",
        "            percentages = [\n",
        "                None,  # No percentage for 1a\n",
        "                None,  # No percentage for 1b\n",
        "                results.get('1c_percentage_new_nouns', results.get('1c_percentage_new_adj', results.get('1c_percentage_new_adv', results.get('1c_percentage_new_verbs', results.get('1c_percentage_new_allpos')))))\n",
        "            ]\n",
        "\n",
        "            for i, (comp_type, count) in enumerate(comparisons):\n",
        "                ws.cell(row=current_row, column=2, value=comp_type)\n",
        "                ws.cell(row=current_row, column=3, value=count)\n",
        "\n",
        "                if percentages[i] is not None:\n",
        "                    ws.cell(row=current_row, column=4, value=f\"{percentages[i]:.1f}%\")\n",
        "\n",
        "                ws.cell(row=current_row, column=5, value=descriptions[comp_type])\n",
        "                current_row += 1\n",
        "\n",
        "            current_row += 1  # Add empty row between categories\n",
        "\n",
        "        # Add summary section\n",
        "        ws.cell(row=current_row, column=1, value=\"SUMMARY\")\n",
        "        current_row += 1\n",
        "\n",
        "        summary_data = [\n",
        "            (\"Total new nouns\", comparison_results['NOUNS']['1c_new_nouns_in_3']),\n",
        "            (\"Total new adjectives\", comparison_results['ADJECTIVES']['1c_new_adj_in_3']),\n",
        "            (\"Total new adverbs\", comparison_results['ADVERBS']['1c_new_adv_in_3']),\n",
        "            (\"Total new verbs\", comparison_results['VERBS']['1c_new_verbs_in_3']),\n",
        "            (\"Total new words (Top-100)\", comparison_results['TOP100_ALL_POS']['1c_new_allpos_in_3'])\n",
        "        ]\n",
        "\n",
        "        for label, value in summary_data:\n",
        "            ws.cell(row=current_row, column=1, value=label)\n",
        "            ws.cell(row=current_row, column=3, value=value)\n",
        "            current_row += 1\n",
        "\n",
        "        # Auto-adjust column widths\n",
        "        for column in ws.columns:\n",
        "            max_length = 0\n",
        "            column_letter = column[0].column_letter\n",
        "            for cell in column:\n",
        "                if cell.value:\n",
        "                    max_length = max(max_length, len(str(cell.value)))\n",
        "            adjusted_width = (max_length + 2)\n",
        "            ws.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "        # Save the workbook\n",
        "        wb.save(excel_file)\n",
        "        print(f\"‚úì Comparison results saved to sheet 'POS_Comparisons' in '{excel_file}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving comparison results to Excel: {e}\")"
      ],
      "metadata": {
        "id": "PpQFF7ygmqXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9. COMPARISON ANALYSIS of WORDS\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"COMPARISON ANALYSIS\")\n",
        "\n",
        "# Check if we have the required data\n",
        "if len(top25_nouns) >= 3 and len(top25_adj) >= 3 and len(top25_adv) >= 3 and len(top25_verbs) >= 3 and len(top100_allpos) >= 3:\n",
        "    # Use the same Excel filename as before\n",
        "    excel_file = generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys()))\n",
        "\n",
        "    # Perform comparisons and save to Excel\n",
        "    comparison_results = compare_pos_lists(top25_nouns, top25_adj, top25_adv, top25_verbs, top100_allpos, excel_file)\n",
        "\n",
        "    print(f\"\\n‚úÖ Comparison analysis completed successfully! Results saved to Excel file: '{excel_file}'.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Not enough data for comparison. Need at least 3 documents with complete POS data.\")"
      ],
      "metadata": {
        "id": "XyU29LSrnCVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 10. Most common Bigrams\n",
        "\n",
        "In this part, **we generate and analyze bigrams** (pairs of consecutive words) from the text of each PDF. Using the nltk library's ngrams function, we calculate the frequency of bigrams in each text and display the top 25 most common bigrams along with their frequency. This helps us identify recurring word combinations in the documents."
      ],
      "metadata": {
        "id": "2wTMxIlhYK3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams  # Import the 'ngrams' function from the nltk library, which helps generate n-grams\n",
        "from collections import Counter  # Import Counter, which counts the occurrences of items\n",
        "\n",
        "# Define a function to get n-grams\n",
        "def get_ngrams(text, n=2): # Default n-gram size set to 2\n",
        "    words = text.split()\n",
        "    # 'text.split()' splits the input text into words by spaces\n",
        "    return Counter(ngrams(words, n))  # Returns a Counter object with n-grams and their counts\n",
        "    # 'ngrams(words, n)' generates a list of n-grams from the words in the text\n",
        "    # 'Counter' counts the frequency of each n-gram in the generated n-gram list\n",
        "\n",
        "# Display top 25 bigrams (n=2) for each text\n",
        "print(\"\\nTop 25 Bigrams in Each Text:\")\n",
        "# Loop through each text and its corresponding file name in the pdf_texts dictionary\n",
        "for filename, text in pdf_texts.items():\n",
        "    # Call the get_ngrams function with n=2 (bigrams) to calculate the frequency of bigrams\n",
        "    bigram_freq = get_ngrams(text, n=2)\n",
        "\n",
        "    # Print the name of the current file\n",
        "    print(f\"\\nTop Bigrams in {filename}:\")\n",
        "\n",
        "    # Loop through the most common 10 bigrams (word pairs) and their frequencies\n",
        "    for bigram, freq in bigram_freq.most_common(25):\n",
        "        # 'bigram' is a tuple of two words, 'freq' is the frequency of that bigram\n",
        "        print(f\"  {' '.join(bigram)}: {freq}\")\n",
        "        # Use 'join' to combine the words in the bigram into a single string for display\n"
      ],
      "metadata": {
        "id": "XHElSXkRv7M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the Bigrams in cleaned (processed text)"
      ],
      "metadata": {
        "id": "YSeqxkiPfB8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save top 25 bigrams for each document to Excel in a new sheet\n",
        "def save_bigrams_to_excel(pdf_texts, excel_file):\n",
        "    try:\n",
        "        # Check if file exists\n",
        "        if os.path.exists(excel_file):\n",
        "            wb = openpyxl.load_workbook(excel_file)\n",
        "            print(f\"‚úì Appending bigrams to existing Excel file: '{excel_file}'\")\n",
        "        else:\n",
        "            wb = Workbook()\n",
        "            # Remove default sheet\n",
        "            wb.remove(wb['Sheet'])\n",
        "            print(f\"‚úì Creating new Excel file for bigrams: '{excel_file}'\")\n",
        "\n",
        "        # Remove existing bigrams sheet if it exists\n",
        "        if 'Top-25 Bigrams' in wb.sheetnames:\n",
        "            del wb['Top-25 Bigrams']\n",
        "\n",
        "        # Create new sheet for bigrams\n",
        "        ws = wb.create_sheet(\"Top-25 Bigrams\")\n",
        "\n",
        "        # Define starting columns for each document\n",
        "        doc_columns = {\n",
        "            0: 'A',  # Document 1\n",
        "            1: 'D',  # Document 2\n",
        "            2: 'G'   # Document 3\n",
        "        }\n",
        "\n",
        "        # Populate the sheet with bigram data\n",
        "        for doc_idx, (filename, text) in enumerate(pdf_texts.items()):\n",
        "            start_col = doc_columns.get(doc_idx, 'A')\n",
        "\n",
        "            # Get bigram frequencies\n",
        "            bigram_freq = get_ngrams(text, n=2)\n",
        "\n",
        "            # Document title\n",
        "            ws[f'{start_col}1'] = filename\n",
        "\n",
        "            # Headers\n",
        "            ws[f'{start_col}2'] = 'Rank'\n",
        "            ws[f'{chr(ord(start_col)+1)}2'] = 'Bigram'\n",
        "            ws[f'{chr(ord(start_col)+2)}2'] = 'Frequency'\n",
        "\n",
        "            current_row = 3\n",
        "\n",
        "            # Add top 25 bigrams\n",
        "            for rank, (bigram, freq) in enumerate(bigram_freq.most_common(25), 1):\n",
        "                bigram_text = ' '.join(bigram)\n",
        "\n",
        "                ws[f'{start_col}{current_row}'] = rank\n",
        "                ws[f'{chr(ord(start_col)+1)}{current_row}'] = bigram_text\n",
        "                ws[f'{chr(ord(start_col)+2)}{current_row}'] = freq\n",
        "                current_row += 1\n",
        "\n",
        "        # Auto-adjust column widths\n",
        "        for column in ws.columns:\n",
        "            max_length = 0\n",
        "            column_letter = column[0].column_letter\n",
        "            for cell in column:\n",
        "                if cell.value:\n",
        "                    max_length = max(max_length, len(str(cell.value)))\n",
        "            adjusted_width = (max_length + 2)\n",
        "            ws.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "        # Save the workbook\n",
        "        wb.save(excel_file)\n",
        "        print(f\"‚úì Top 25 bigrams saved to sheet 'Top-25 Bigrams' in '{excel_file}'\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving bigrams to Excel: {e}\")\n",
        "        return False\n",
        "\n",
        "# Define a function to get n-grams from preprocessed text\n",
        "def get_ngrams(text, n=2):\n",
        "    text = CleanText(text)                  # Clean the text from noise of header and footer\n",
        "    words = preprocess_text(text)  # Preprocess to remove stop words and lowercase\n",
        "    return Counter(ngrams(words, n))\n",
        "\n",
        "# Display top 25 bigrams after stop words removal\n",
        "print(\"\\nTop 25 Bigrams in Each Text (Without Stop Words, HeaderFooter):\")\n",
        "for filename, text in pdf_texts.items():\n",
        "    bigram_freq = get_ngrams(text, n=2)  # Get bigram frequencies\n",
        "    print(f\"\\nTop Bigrams in {filename}:\")\n",
        "    for bigram, freq in bigram_freq.most_common(25):\n",
        "        print(f\"  {' '.join(bigram)}: {freq}\")\n",
        "\n",
        "# Save bigrams to Excel\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SAVING BIGRAMS TO EXCEL\")\n",
        "\n",
        "# Generate the Excel filename and save bigrams\n",
        "if pdf_texts:\n",
        "    excel_file = generate_exported_excelfile(list(pdf_texts.keys())[0], list(pdf_texts.keys()))\n",
        "    bigrams_saved = save_bigrams_to_excel(pdf_texts, excel_file)\n",
        "\n",
        "    if bigrams_saved:\n",
        "        print(f\"‚úÖ Bigrams successfully saved to: {excel_file}. Check the 'Top-25 Bigrams' sheet for bigram frequencies\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to save bigrams to Excel\")\n",
        "else:\n",
        "    print(\"‚ùå No text data available to extract bigrams\")"
      ],
      "metadata": {
        "id": "QxeNEF7VgkVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Excel file\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZaqFoiyO7YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Excel file from Colab\n",
        "print(f\"\\nüì• Download the Excel file:\")\n",
        "files.download(excel_file)\n"
      ],
      "metadata": {
        "id": "7WYp2rXxdq3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}